{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive Bayes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7qPwcmL53CJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bab355be-c465-4660-d8ad-efaa47ba8eed"
      },
      "source": [
        "import shutil\n",
        "shutil.copy2(\"/content/drive/My Drive/ML Offline/Data.zip?dl=0\", \"/content\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/Data.zip?dl=0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNez1RT49JVD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "outputId": "5602fe36-5b56-4ae9-ecbf-5ab3a7b3ecee"
      },
      "source": [
        "!unzip /content/drive/\"My Drive\"/\"ML Offline\"/Data.zip?dl=0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/ML Offline/Data.zip?dl=0\n",
            "   creating: Data/Test/\n",
            "  inflating: Data/Test/3d_Printer.xml  \n",
            "  inflating: Data/Test/Anime.xml     \n",
            "  inflating: Data/Test/Arduino.xml   \n",
            "  inflating: Data/Test/Astronomy.xml  \n",
            "  inflating: Data/Test/Biology.xml   \n",
            "  inflating: Data/Test/Chess.xml     \n",
            "  inflating: Data/Test/Coffee.xml    \n",
            "  inflating: Data/Test/Cooking.xml   \n",
            "  inflating: Data/Test/Law.xml       \n",
            "  inflating: Data/Test/Space.xml     \n",
            "  inflating: Data/Test/Windows_Phone.xml  \n",
            "  inflating: Data/Test/Wood_Working.xml  \n",
            "  inflating: Data/topics.txt         \n",
            "   creating: Data/Training/\n",
            "  inflating: Data/Training/3d_Printer.xml  \n",
            "  inflating: Data/Training/Anime.xml  \n",
            "  inflating: Data/Training/Arduino.xml  \n",
            "  inflating: Data/Training/Astronomy.xml  \n",
            "  inflating: Data/Training/Biology.xml  \n",
            "  inflating: Data/Training/Chess.xml  \n",
            "  inflating: Data/Training/Coffee.xml  \n",
            "  inflating: Data/Training/Cooking.xml  \n",
            "  inflating: Data/Training/Law.xml   \n",
            "  inflating: Data/Training/Space.xml  \n",
            "  inflating: Data/Training/Windows_Phone.xml  \n",
            "  inflating: Data/Training/Wood_Working.xml  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIJc6e689Ml5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.ticker as ticker\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from sys import stdout\n",
        "import os\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2-Vk2YV9VWh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "871a1ac4-2ada-4668-9f7d-da4956f59d53"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTzCBIlh9XaA"
      },
      "source": [
        "def preprocess(text):\n",
        "  #Lowercase the text\n",
        "  text = text.lower()\n",
        "\n",
        "  #Number Removal\n",
        "  text = re.sub(r'[-+]?\\d+', '', text)\n",
        "\n",
        "  #Remove punctuations\n",
        "  text=text.translate((str.maketrans('','',string.punctuation)))\n",
        "\n",
        "  #Tokenize\n",
        "  text = word_tokenize(text)\n",
        "\n",
        "  #Remove stopwords\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  text = [word for word in text if not word in stop_words]\n",
        "\n",
        "  #Lemmatize tokens\n",
        "  lemmatizer=WordNetLemmatizer()\n",
        "  text = [lemmatizer.lemmatize(word) for word in text]\n",
        "\n",
        "  #Stemming tokens\n",
        "  stemmer= PorterStemmer()\n",
        "  text = [stemmer.stem(word) for word in text]\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv-LbdQQ9bVp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "80947c81-bb14-4132-e15d-2435a1a92b2c"
      },
      "source": [
        "data_path = \"/content/Data/Training\"\n",
        "data_files_list = os.listdir(data_path)\n",
        "data_files_list.remove('3d_Printer.xml')\n",
        "#data_files_list = ['Coffee.xml', 'Arduino.xml', 'Anime.xml']\n",
        "training_data_dict_list = []\n",
        "valid_data_dict_list = []\n",
        "test_data_dict_list = []\n",
        "for data_file in tqdm(data_files_list):\n",
        "  data_file_path = data_path + \"/\" + data_file\n",
        "  topic = data_file.split(\".\")[0]\n",
        "  #print(topic)\n",
        "  with open(data_file_path, \"r\") as f:\n",
        "    line_count = 0\n",
        "    for line in f:\n",
        "      line = line.lstrip()\n",
        "      if line.startswith(\"<row\"):\n",
        "        #line_count += 1\n",
        "        data_dict = {}\n",
        "        soup = BeautifulSoup(line, \"xml\")\n",
        "        body_temp = soup.find(\"row\")[\"Body\"]\n",
        "        soup2 = BeautifulSoup(body_temp, \"html\")\n",
        "        body = soup2.get_text().replace(\"\\n\", \" \")\n",
        "        if body.strip() == \"\":\n",
        "          continue\n",
        "        words_list = preprocess(body)\n",
        "        if len(words_list) == 0:\n",
        "          continue\n",
        "        line_count += 1\n",
        "        data_dict['doc'] = words_list\n",
        "        data_dict['topic'] = topic\n",
        "        if line_count <= 500:\n",
        "          training_data_dict_list.append(data_dict)\n",
        "        elif line_count > 500 and line_count <=700:\n",
        "          valid_data_dict_list.append(data_dict)\n",
        "        elif line_count > 700 and line_count <= 1200:\n",
        "          test_data_dict_list.append(data_dict)   \n",
        "      if line_count >= 1200: \n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:38<00:00,  3.52s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da2wU77n9gOi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "4869e8d9-3ef8-4333-a868-c181657108c3"
      },
      "source": [
        "print(len(training_data_dict_list))\n",
        "print(len(test_data_dict_list))\n",
        "print(len(valid_data_dict_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5500\n",
            "5500\n",
            "2200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKB1W6vc9nf3"
      },
      "source": [
        "with open(\"train.json\", \"w\") as f:\n",
        "  json.dump(training_data_dict_list, f)\n",
        "\n",
        "with open(\"test.json\", \"w\") as f:\n",
        "  json.dump(test_data_dict_list, f)\n",
        "\n",
        "with open (\"valid.json\", \"w\") as f:\n",
        "  json.dump(valid_data_dict_list, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7jYZvQb9v5F"
      },
      "source": [
        "with open(\"train.json\", \"r\") as f:\n",
        "  train_data = json.load(f)\n",
        "\n",
        "with open(\"valid.json\", \"r\") as f:\n",
        "  valid_data = json.load(f)\n",
        "\n",
        "with open(\"test.json\", \"r\") as f:\n",
        "  test_data = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W47niPUt-ri-"
      },
      "source": [
        "def get_vocabulary(train_data):\n",
        "  all_train_words = []\n",
        "  for d in train_data:\n",
        "    for word in d['doc']:\n",
        "      all_train_words.append(word)\n",
        "  vocabulary = np.unique(all_train_words)\n",
        "  return vocabulary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWulUaah-5kf"
      },
      "source": [
        "def get_classes(train_data):\n",
        "  classes = np.unique([d['topic'] for d in train_data])\n",
        "  return classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_XiMtkE_wjc"
      },
      "source": [
        "def calculate_total_words_in_each_class(train_data, classes):\n",
        "  total_words_in_class = {} #Nck\n",
        "  for cls in classes:\n",
        "    total_words_in_class[cls] = 0\n",
        "  for d in tqdm(train_data):\n",
        "    total_words_in_class[d['topic']] += len(d['doc'])\n",
        "  return total_words_in_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enFCaXd6AtO_"
      },
      "source": [
        "def calculate_number_of_each_word_in_each_class(train_data, classes, vocabulary):\n",
        "  word_in_each_class = {}\n",
        "  for word in vocabulary:\n",
        "    word_in_each_class[word] = {}\n",
        "    for cls in classes:\n",
        "      word_in_each_class[word][cls] = 0\n",
        "\n",
        "  for word in tqdm(vocabulary):\n",
        "    for d in train_data:\n",
        "      word_in_each_class[word][d['topic']] += d['doc'].count(word)\n",
        "  return word_in_each_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaRdRP6haOOQ"
      },
      "source": [
        "def renew_probabilities(classes, vocabulary):\n",
        "  p_word_class = {}\n",
        "  for word in vocabulary:\n",
        "    p_word_class[word] = {}\n",
        "    for cls in classes:\n",
        "      p_word_class[word][cls] = 0\n",
        "  return p_word_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU-HkD3ZCcow"
      },
      "source": [
        "def train(alpha, vocabulary, classes, word_in_each_class, total_words_in_class):\n",
        "  V = len(vocabulary)\n",
        "  p_word_class = renew_probabilities(classes, vocabulary)\n",
        "  for word in tqdm(vocabulary):\n",
        "    for cls in classes:\n",
        "      p_word_class[word][cls] = np.longdouble((np.longdouble(word_in_each_class[word][cls] + alpha))/(np.longdouble(total_words_in_class[cls] + (alpha * V))))\n",
        "  return p_word_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATnKWEJYh4D7"
      },
      "source": [
        "def predict(predict_data, p_word_class, classes, p_unknown_word):\n",
        "  probability_count = {}\n",
        "  for cls in classes:\n",
        "    probability_count[cls] = 1\n",
        "  words = predict_data['doc']\n",
        "  real_topic = predict_data['topic']\n",
        "\n",
        "  for cls in classes:\n",
        "    for word in words:\n",
        "      if p_word_class.get(word) != None:\n",
        "        probability_count[cls] = probability_count[cls] * p_word_class[word][cls]\n",
        "      else:\n",
        "        probability_count[cls] = probability_count[cls] * p_unknown_word[cls]\n",
        "\n",
        "  predicted_topic = \"\"\n",
        "  max_p = -999999\n",
        "  for cls in classes:\n",
        "    if probability_count[cls] > max_p:\n",
        "      max_p = probability_count[cls]\n",
        "      predicted_topic = cls \n",
        "  \n",
        "  return predicted_topic\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmVEGxo5X_V6"
      },
      "source": [
        "classes = get_classes(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0jAfGjFgg2D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "2958b0f0-5e22-4b6d-ea84-99e4948f5165"
      },
      "source": [
        "total_words_in_class = calculate_total_words_in_each_class(train_data, classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5500/5500 [00:00<00:00, 536805.32it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifRaJvG2gtMe"
      },
      "source": [
        "vocabulary = get_vocabulary(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul9vQYIZg4VK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "68a59676-074e-4a8c-9df2-0de9f934e3d4"
      },
      "source": [
        "word_in_each_class = calculate_number_of_each_word_in_each_class(train_data, classes, vocabulary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20620/20620 [03:18<00:00, 103.76it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9Ocsxc7bJrQ"
      },
      "source": [
        "def calculate_accuracy(test_data, p_word_class, classes, p_unknown_word):\n",
        "  correct_count = 0\n",
        "  for data in tqdm(test_data):\n",
        "    real = str(data['topic'])\n",
        "\n",
        "    predicted = str(predict(data, p_word_class, classes, p_unknown_word))\n",
        "\n",
        "    if real == predicted:\n",
        "      correct_count += 1\n",
        "  \n",
        "  accuracy = (correct_count/len(test_data)) * 100\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9m5_narmJrj"
      },
      "source": [
        "alpha = 0.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyLDIoS8hki8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "99df5115-4197-4524-9f11-537f9fddf702"
      },
      "source": [
        "p_word_class = train(alpha, vocabulary, classes, word_in_each_class, total_words_in_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20620/20620 [00:00<00:00, 30183.97it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpJXuRPllt-2"
      },
      "source": [
        "p_unknown_word = {}\n",
        "for cls in classes:\n",
        "  p_unknown_word[cls] = alpha/(total_words_in_class[cls] + (alpha * len(vocabulary)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbUzZPyEks8P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "f4b252bf-d4ad-4c5c-9a4c-bec7fc71a227"
      },
      "source": [
        "calculate_accuracy(valid_data, p_word_class, classes, p_unknown_word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2200/2200 [00:01<00:00, 1873.48it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "91.31818181818183"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kfq_dUudiuJo"
      },
      "source": [
        "small_test_sets = {}\n",
        "for i in range(1, 51):\n",
        "  small_test_sets[i] = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygTmuZFUXb2V"
      },
      "source": [
        "with open(\"test.json\", \"r\") as f:\n",
        "  data = json.load(f)\n",
        "  line_count = 0\n",
        "  test_set_count = 1\n",
        "  for line in data:\n",
        "    line_count += 1\n",
        "    small_test_sets[test_set_count].append(line)\n",
        "    if line_count % 10 == 0:\n",
        "      test_set_count += 1\n",
        "    if line_count % 500 == 0:\n",
        "      test_set_count = 1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awVYA5cREYHP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "86be78e4-f5fa-453e-966f-c319c1d20aa6"
      },
      "source": [
        "accuracy_list = []\n",
        "for i in range(1, 51):\n",
        "  test_data = small_test_sets[i]\n",
        "  accuracy = calculate_accuracy(test_data, p_word_class, classes, p_unknown_word)\n",
        "  accuracy_list.append(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 110/110 [00:00<00:00, 1469.38it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 2069.00it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 2057.87it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1923.05it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1495.29it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1585.21it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1678.60it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 2013.26it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1903.93it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1879.07it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1894.29it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1780.92it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1754.20it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1712.06it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1675.77it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1777.49it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1930.29it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1737.69it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1430.16it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 2103.28it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 2083.32it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1790.33it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1870.26it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 2369.05it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 2314.46it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 2104.85it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 2201.91it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 2161.57it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1655.28it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1984.05it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1888.91it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1900.56it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1620.32it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1485.62it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1939.93it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1965.81it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1941.03it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 2074.61it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1604.79it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1980.53it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1646.50it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1772.07it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1906.57it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1843.97it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 2103.72it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1857.28it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1689.13it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1530.14it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 2263.55it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 1909.50it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p52Rosx3E3EE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "2778ea2f-9f37-49c9-95b1-e394acb75715"
      },
      "source": [
        "for i in range(len(accuracy_list)):\n",
        "  print(str(i+1) + \": \" + str(accuracy_list[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1: 92.72727272727272\n",
            "2: 95.45454545454545\n",
            "3: 96.36363636363636\n",
            "4: 90.0\n",
            "5: 96.36363636363636\n",
            "6: 93.63636363636364\n",
            "7: 95.45454545454545\n",
            "8: 93.63636363636364\n",
            "9: 90.0\n",
            "10: 90.9090909090909\n",
            "11: 92.72727272727272\n",
            "12: 95.45454545454545\n",
            "13: 90.0\n",
            "14: 88.18181818181819\n",
            "15: 90.9090909090909\n",
            "16: 90.0\n",
            "17: 91.81818181818183\n",
            "18: 92.72727272727272\n",
            "19: 91.81818181818183\n",
            "20: 88.18181818181819\n",
            "21: 84.54545454545455\n",
            "22: 97.27272727272728\n",
            "23: 94.54545454545455\n",
            "24: 93.63636363636364\n",
            "25: 89.0909090909091\n",
            "26: 89.0909090909091\n",
            "27: 87.27272727272727\n",
            "28: 90.0\n",
            "29: 87.27272727272727\n",
            "30: 94.54545454545455\n",
            "31: 90.9090909090909\n",
            "32: 93.63636363636364\n",
            "33: 92.72727272727272\n",
            "34: 94.54545454545455\n",
            "35: 90.0\n",
            "36: 92.72727272727272\n",
            "37: 90.0\n",
            "38: 90.9090909090909\n",
            "39: 96.36363636363636\n",
            "40: 96.36363636363636\n",
            "41: 89.0909090909091\n",
            "42: 95.45454545454545\n",
            "43: 90.0\n",
            "44: 90.0\n",
            "45: 93.63636363636364\n",
            "46: 93.63636363636364\n",
            "47: 89.0909090909091\n",
            "48: 88.18181818181819\n",
            "49: 88.18181818181819\n",
            "50: 94.54545454545455\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYaWKjzzE5VM"
      },
      "source": [
        "temp_dict = {}\n",
        "temp_dict['NB'] = accuracy_list\n",
        "with open(\"nb_acc.json\", \"w\") as f:\n",
        "  json.dump(temp_dict, f)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRnG-Wj4HAww",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c3854737-6e4c-4005-c727-3dac495bea12"
      },
      "source": [
        "shutil.copy2(\"/content/nb_acc.json\", \"/content/drive/My Drive/ML Offline\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/ML Offline/nb_acc.json'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvLjD55dHNdK"
      },
      "source": [
        "#Calculating t_stat\n",
        "nb_acc = accuracy_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BswjZqa6IHnf"
      },
      "source": [
        "shutil.copy2(\"/content/drive/My Drive/ML Offline/knn_acc.json\", \"/content\")\n",
        "with open(\"knn_acc.json\", \"r\") as f:\n",
        "  knn_acc = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "850NmamdWqZQ"
      },
      "source": [
        "knn_acc = knn_acc['knn']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgEHO4GxWTcX"
      },
      "source": [
        "with open(\"nb_acc.json\", \"r\") as f:\n",
        "  nb_acc = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmqW2Ej9XEjl"
      },
      "source": [
        "nb_acc = nb_acc['NB']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hp_alOdqI03i"
      },
      "source": [
        "nb_acc = np.array(nb_acc)\n",
        "knn_acc = np.array(knn_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5vFLSLVIiew"
      },
      "source": [
        "from scipy import stats\n",
        "t_stat, p_value = stats.ttest_ind(nb_acc , knn_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTnWswYrWgAN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "04a9db5c-c2b5-4be8-971f-3fee30cbdf28"
      },
      "source": [
        "print(t_stat)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "29.271637869313974\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpYBCAuVXMmp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "37613e29-91b6-4e27-df11-573b49647da7"
      },
      "source": [
        "print(p_value)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.0336649885380764e-50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhaSmdSDXOWF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}